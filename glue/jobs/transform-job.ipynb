{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "####  Set up and start your interactive session\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import functions as F\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 65479d7d-faf3-46a6-8ca4-62a3767725a7.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 65479d7d-faf3-46a6-8ca4-62a3767725a7.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 65479d7d-faf3-46a6-8ca4-62a3767725a7.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 65479d7d-faf3-46a6-8ca4-62a3767725a7.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='chicago_crime_cleanet_parquet_database', table_name='chicago_crime_cleaned_parquet', useCatalogSchema=True)\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- id: long\n|-- case_number: string\n|-- date: timestamp\n|-- block: string\n|-- iucr: string\n|-- primary_type: string\n|-- description: string\n|-- location_description: string\n|-- arrest: boolean\n|-- domestic: boolean\n|-- beat: long\n|-- district: long\n|-- ward: double\n|-- community_area: double\n|-- fbi_code: string\n|-- x_coordinate: double\n|-- y_coordinate: double\n|-- year: long\n|-- updated_on: timestamp\n|-- latitude: double\n|-- longitude: double\n|-- location: string\n|-- __index_level_0__: long\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Convert the DynamicFrame to a Spark DataFrame\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = dyf.toDF()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Filter, Aggregate and rank crimes per week per community_area",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType\n\n# Assuming 'date' is the column that contains the date information\ndf_filtered = df.filter((F.col('date') > '2019-12-31') & (F.col('date') < '2023-01-01'))\n\n# Find the last date in the DataFrame\nlast_date = df_filtered.agg(F.max(\"date\")).collect()[0][0]\n\n# Create a new column with the day difference between each date and the last date\ndf_filtered = df_filtered.withColumn(\"days_from_last\", F.datediff(F.lit(last_date), \"date\"))\n\n# Calculate the new window based on days from last\ndf_filtered = df_filtered.withColumn(\"window_start_days_from_last\", (F.floor(df_filtered.days_from_last / 7) * 7).cast(IntegerType()))\ndf_filtered = df_filtered.withColumn(\"window_end_days_from_last\", (F.floor(df_filtered.days_from_last / 7) * 7 + 6).cast(IntegerType()))\n\n# Calculate window start and end dates\ndf_filtered = df_filtered.withColumn(\"window_end\", F.expr(f\"date_add(to_date('{last_date}'), -window_start_days_from_last)\"))\ndf_filtered = df_filtered.withColumn(\"window_start\", F.expr(f\"date_add(to_date('{last_date}'), -window_end_days_from_last)\"))\n\n# Now proceed with 7-day window aggregation\ndf_aggregated = df_filtered.groupBy(\"window_start\", \"window_end\", \"community_area\") \\\n                           .agg(F.count(\"*\").alias(\"num_of_crimes\"))\n\n# Create a window specification\nwindowSpec = Window.partitionBy(\"window_start\", \"window_end\").orderBy(F.desc(\"num_of_crimes\"))\n\n# Use the rank function over the window specification\ndf_ranked = df_aggregated.withColumn(\"rank\", F.rank().over(windowSpec))\n\n# Show the DataFrame\ndf_ranked.sort(\"window_end\", ascending=False).show()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------+----------+--------------+-------------+----+\n|window_start|window_end|community_area|num_of_crimes|rank|\n+------------+----------+--------------+-------------+----+\n|  2022-12-25|2022-12-31|          19.0|           64|  19|\n|  2022-12-25|2022-12-31|          23.0|           83|  10|\n|  2022-12-25|2022-12-31|           6.0|           67|  18|\n|  2022-12-25|2022-12-31|          71.0|           93|   6|\n|  2022-12-25|2022-12-31|          49.0|           83|  10|\n|  2022-12-25|2022-12-31|          66.0|           76|  14|\n|  2022-12-25|2022-12-31|          67.0|           71|  17|\n|  2022-12-25|2022-12-31|           8.0|          149|   2|\n|  2022-12-25|2022-12-31|          44.0|           96|   5|\n|  2022-12-25|2022-12-31|          46.0|           88|   8|\n|  2022-12-25|2022-12-31|          68.0|           84|   9|\n|  2022-12-25|2022-12-31|          69.0|           81|  12|\n|  2022-12-25|2022-12-31|          29.0|           78|  13|\n|  2022-12-25|2022-12-31|          24.0|           74|  15|\n|  2022-12-25|2022-12-31|           2.0|           73|  16|\n|  2022-12-25|2022-12-31|          25.0|          164|   1|\n|  2022-12-25|2022-12-31|          38.0|           62|  21|\n|  2022-12-25|2022-12-31|          43.0|          133|   3|\n|  2022-12-25|2022-12-31|          28.0|          114|   4|\n|  2022-12-25|2022-12-31|          32.0|           91|   7|\n+------------+----------+--------------+-------------+----+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_ranked = df_ranked.withColumn(\"community_area\", F.col(\"community_area\").cast(\"int\"))\n\ndf_ranked.sort(df_ranked.window_start.desc()).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------+----------+--------------+-------------+----+\n|window_start|window_end|community_area|num_of_crimes|rank|\n+------------+----------+--------------+-------------+----+\n|  2022-12-25|2022-12-31|            19|           64|  19|\n|  2022-12-25|2022-12-31|            49|           83|  10|\n|  2022-12-25|2022-12-31|             6|           67|  18|\n|  2022-12-25|2022-12-31|            71|           93|   6|\n|  2022-12-25|2022-12-31|            23|           83|  10|\n|  2022-12-25|2022-12-31|            66|           76|  14|\n|  2022-12-25|2022-12-31|            67|           71|  17|\n|  2022-12-25|2022-12-31|             8|          149|   2|\n|  2022-12-25|2022-12-31|            44|           96|   5|\n|  2022-12-25|2022-12-31|            46|           88|   8|\n|  2022-12-25|2022-12-31|            68|           84|   9|\n|  2022-12-25|2022-12-31|            69|           81|  12|\n|  2022-12-25|2022-12-31|            29|           78|  13|\n|  2022-12-25|2022-12-31|            24|           74|  15|\n|  2022-12-25|2022-12-31|             2|           73|  16|\n|  2022-12-25|2022-12-31|            25|          164|   1|\n|  2022-12-25|2022-12-31|            38|           62|  21|\n|  2022-12-25|2022-12-31|            43|          133|   3|\n|  2022-12-25|2022-12-31|            28|          114|   4|\n|  2022-12-25|2022-12-31|            32|           91|   7|\n+------------+----------+--------------+-------------+----+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_ranked.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- window_start: date (nullable = true)\n |-- window_end: date (nullable = true)\n |-- community_area: integer (nullable = true)\n |-- num_of_crimes: long (nullable = false)\n |-- rank: integer (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Write the data in the DynamicFrame to S3",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import boto3\n\ndef delete_objects_with_prefix(bucket_name, prefix):\n    s3 = boto3.client('s3')\n    \n    # Initialize variables for pagination\n    continuation_token = None\n    delete_keys = {'Objects': []}\n    \n    while True:\n        # List objects with pagination support\n        list_kwargs = {'Bucket': bucket_name, 'Prefix': prefix}\n        if continuation_token:\n            list_kwargs['ContinuationToken'] = continuation_token\n            \n        objects_to_delete = s3.list_objects_v2(**list_kwargs)\n        \n        # Prepare the list of objects to delete\n        for obj in objects_to_delete.get('Contents', []):\n            delete_keys['Objects'].append({'Key': obj['Key']})\n        \n        # Delete the objects\n        if delete_keys['Objects']:\n            s3.delete_objects(Bucket=bucket_name, Delete=delete_keys)\n            delete_keys = {'Objects': []}  # Reset the delete_keys list\n        \n        # Check for more objects to list (pagination)\n        if objects_to_delete.get('IsTruncated'):\n            continuation_token = objects_to_delete['NextContinuationToken']\n        else:\n            break\n\n# Usage\nbucket_name = 'chicagocrimedata-chalenge'\nfolder_path = 'data/chicago_crime_rank_database/chicago_crime_rank_parquet/'\n\ndelete_objects_with_prefix(bucket_name, folder_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\n\ndyf = DynamicFrame.fromDF(df_ranked, glueContext, \"chicago_crime_rank_dynamic_frame\")\n\ndyf.printSchema()\n\ns3output = glueContext.getSink(\n  path=\"s3://chicagocrimedata-chalenge/data/chicago_crime_rank_database/chicago_crime_rank_parquet/\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[\"community_area\", \"window_start\"],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"chicago_crime_rank_parquet_database\", catalogTableName=\"chicago_crime_rank\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- community_area: double\n|-- num_of_crimes: long\n|-- rank: int\n|-- window_start: timestamp\n|-- window_end: timestamp\n\n<awsglue.dynamicframe.DynamicFrame object at 0x7f4bf4530e50>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}