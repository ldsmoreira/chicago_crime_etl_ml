{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		},
		"toc-showtags": false
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "import math\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport boto3\nfrom pydantic import BaseModel, Field, ValidationError\nfrom datetime import datetime\nfrom typing import Optional",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Set up and start your interactive session\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Get parameters from previous job",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "args = getResolvedOptions(sys.argv, ['bucket_name', 'object'])\nprint(args['bucket_name'])\nprint(args['object'])",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Schema Enforcement using Pydantic",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Define the schema for Chicago Crime data\nclass ChicagoCrimeSchema(BaseModel):\n    ID: Optional[int]\n    Case_Number: Optional[str]\n    Date: Optional[datetime]\n    Block: Optional[str]\n    IUCR: Optional[str]\n    Primary_Type: Optional[str]\n    Description: Optional[str]\n    Location_Description: Optional[str]\n    Arrest: Optional[bool]\n    Domestic: Optional[bool]\n    Beat: Optional[int]\n    District: Optional[int]\n    Ward: Optional[int]\n    Community_Area: Optional[int]\n    FBI_Code: Optional[str]\n    X_Coordinate: Optional[int]\n    Y_Coordinate: Optional[int]\n    Year: Optional[int]\n    Updated_On: Optional[datetime]\n    Latitude: Optional[float]\n    Longitude: Optional[float]\n    Location: Optional[str]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Download Raw data, address some data inconsistencies and send it back to S3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "s3 = boto3.client('s3')\n\n# Extract file and bucket information from the Lambda event\nfile_name = args['object'].split('/')[-1]\nbucket_name = args['bucket_name']\n\n# Download the file from S3\ns3.download_file(bucket_name, args['object'], f'{file_name}')\n\n# Read the CSV file and handle data types\ndf = pd.read_csv(f'{file_name}', parse_dates=['Date', 'Updated On'], low_memory=False)\ndf.replace({float('nan'): None}, inplace=True)\ndf['Date'] = pd.to_datetime(df['Date'], errors='coerce', format='%m/%d/%Y %I:%M:%S %p')\ndf['Updated On'] = pd.to_datetime(df['Updated On'], errors='coerce', format='%m/%d/%Y %I:%M:%S %p')\n\n# Validate each row against the schema\ninvalid_rows = []\nfor index, row in df.iterrows():\n    row_dict = row.rename(lambda x: x.replace(' ', '_').replace('-', '_')).to_dict()\n    try:\n        ChicagoCrimeSchema(**row_dict)\n    except ValidationError:\n        print(row_dict)\n        invalid_rows.append(index)\n\n# Remove invalid rows\ndf.drop(index=invalid_rows, inplace=True)\n\n# Standardize column names\ndf.columns = df.columns.str.lower().str.replace(' ', '_')\n\n# Convert DataFrame to PyArrow Table\ntable = pa.Table.from_pandas(df)\n\n# Save the table as a Parquet file\npq.write_table(table, '/tmp/preprocessed_chicago_crime_data.parquet')\n\n# Upload the Parquet file back to S3\ns3.upload_file('/tmp/preprocessed_chicago_crime_data.parquet', \n               bucket_name, \n               'data/chicago_crime_database/chicago_crime_parquet/preprocessed_chicago_crime_data.parquet')",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}